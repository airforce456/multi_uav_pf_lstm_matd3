# Default Configuration

# Experiment Settings
run_name: "PF_LSTM_MATD3_default"
seed: 42
device: "auto" # "auto", "cpu", "cuda"

# Environment Settings
env:
  n_uavs: 3
  n_obstacles: 2
  max_steps: 500
  dt: 0.1 # time step for simulation
  arena_size: [10, 10, 5] # X, Y, Z
  formation_switch_enabled: true

  # Potential Field Reward Coefficients
  reward_weights:
    goal: 1.0         # Attraction to goal
    formation: 0.5    # Attraction to formation slot
    obstacle: -1.5    # Repulsion from obstacles
    uav: -1.0         # Repulsion from other UAVs
    collision: -200.0   # Penalty for collision
    goal_reached: 200.0 # Reward for reaching goal

  # Distances for potential field calculations
  distances:
    obstacle_repu_dist: 1.5
    uav_repu_dist: 1.0
    goal_tolerance: 0.5
    formation_tolerance: 0.2

# Agent & Algorithm Settings
agent:
  gamma: 0.99
  tau: 0.005
  policy_freq: 2 # Delayed actor update frequency
  lr_actor: 0.0001
  lr_critic: 0.001

  # LSTM Settings
  lstm:
    seq_len: 8
    hidden_size: 64

  # Actor-Critic Network Hidden Sizes
  actor_hidden_size: 128
  critic_hidden_size: 128

  # Action Noise (for exploration during training)
  expl_noise: 0.1
  policy_noise: 0.2
  noise_clip: 0.5

# Training Settings
train:
  total_episodes: 5000
  buffer_size: 100000
  batch_size: 256
  start_train_episodes: 10 # Episodes to run before starting training

  # Prioritized Experience Replay (PER)
  per:
    enabled: true
    alpha: 0.6  # Priority exponent
    beta_start: 0.4 # Initial importance sampling exponent
    beta_end: 1.0   # Final importance sampling exponent
    beta_anneal_episodes: 4000

  # Logging and Saving
  log_interval: 10 # episodes
  save_interval: 500 # episodes